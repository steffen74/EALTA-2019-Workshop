{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/home/steffen/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9249897599220276}]\n"
     ]
    }
   ],
   "source": [
    "# example call of the pipeline function\n",
    "# to perform named entity recognition (NER) on a text\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\")\n",
    "sequence = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very close to the Manhattan Bridge.\"\n",
    "print(nlp(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/home/steffen/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8445992469787598, 0.11197395622730255, 0.043426770716905594]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example for zero shot classification\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to write your own language. This course will be aimed at teaching you how to use languages that represent the'},\n",
       " {'generated_text': 'In this course, we will teach you how to run a successful run like Run a Flugger on a flat screen. First of all, let'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example for text generation with an explicit model\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete.\n",
      "\n",
      "Padding token set to EOS token.\n",
      "\n",
      "Processing the following prompts:\n",
      "- I've been waiting for a HuggingFace course my whole life,\n",
      "- I hate this so much,\n",
      "\n",
      "Tokenizing inputs...\n",
      "Tokens (input IDs): tensor([[   40,  1053,   587,  4953,   329,   257, 12905,  2667, 32388,  1781,\n",
      "           616,  2187,  1204,    11],\n",
      "        [   40,  5465,   428,   523,   881,    11, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256]])\n",
      "Attention masks: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Tokenization complete.\n",
      "\n",
      "Generating text...\n",
      "Tokens (output IDs): tensor([[   40,  1053,   587,  4953,   329,   257, 12905,  2667, 32388,  1781,\n",
      "           616,  2187,  1204,    11,   290,   314,  1101,   523,  9675,   314,\n",
      "           750,    13,   314,  1101,   523,  3772,   284,   307,  1498,   284,\n",
      "          2648,   616,  1998,   351,   345,    13,   198,   198,    40,  1101,\n",
      "           523,  3772,   284,   307,  1498,   284,  2648,   616,  1998,   351],\n",
      "        [   40,  5465,   428,   523,   881,    11, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256,   464,   717,   640,   314,  2497,   262,\n",
      "           649,   366,   464, 21276,  5542,     1, 12268,    11,   314,   373,\n",
      "           523,  6568,   284,   766,   262,   717, 12268,   329,   262,  7865,\n",
      "          1622,   286,   262,   905,    13,   314,   373,   523,  6568,   284]])\n",
      "Generation complete.\n",
      "\n",
      "Decoding outputs...\n",
      "Decoded texts:\n",
      "Completion to Prompt 1:\n",
      "I've been waiting for a HuggingFace course my whole life, and I'm so glad I did. I'm so happy to be able to share my experience with you.\n",
      "\n",
      "I'm so happy to be able to share my experience with\n",
      "\n",
      "Completion to Prompt 2:\n",
      "I hate this so much,The first time I saw the new \"The Walking Dead\" trailer, I was so excited to see the first trailer for the upcoming season of the show. I was so excited to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the tokenizer and model\n",
    "checkpoint = \"gpt2\"\n",
    "print(\"Initializing tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "print(\"Initialization complete.\\n\")\n",
    "\n",
    "# Set padding token if not preset\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Set EOS token as padding token\n",
    "    print(\"Padding token set to EOS token.\\n\")\n",
    "\n",
    "# Define sample prompts\n",
    "prompts = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life,\",\n",
    "    \"I hate this so much,\"\n",
    "]\n",
    "print(\"Processing the following prompts:\")\n",
    "for prompt in prompts:\n",
    "    print(f\"- {prompt}\")\n",
    "print()\n",
    "\n",
    "# Tokenize inputs\n",
    "print(\"Tokenizing inputs...\")\n",
    "inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(\"Tokens (input IDs):\", inputs[\"input_ids\"])\n",
    "print(\"Attention masks:\", inputs[\"attention_mask\"])\n",
    "print(\"Tokenization complete.\\n\")\n",
    "\n",
    "# Generate text\n",
    "print(\"Generating text...\")\n",
    "generated_outputs = model.generate(inputs[\"input_ids\"], max_length=50)\n",
    "print(\"Tokens (output IDs):\", generated_outputs)\n",
    "print(\"Generation complete.\\n\")\n",
    "\n",
    "# Decode and display the outputs\n",
    "print(\"Decoding outputs...\")\n",
    "generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in generated_outputs]\n",
    "print(\"Decoded texts:\")\n",
    "for i, text in enumerate(generated_texts, 1):\n",
    "    print(f\"Completion to Prompt {i}:\\n{text}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
